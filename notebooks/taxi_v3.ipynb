{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'+', b'-', b'-', b'-', b'-', b'-', b'-', b'-', b'-', b'-', b'+'],\n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'R', b':', b' ', b'|', b' ', b':', b' ', b':', b'G'],\n",
       "       [b' ', b':', b' ', b'|', b' ', b':', b' ', b':', b' '],\n",
       "       [b' ', b':', b' ', b':', b' ', b':', b' ', b':', b' '],\n",
       "       [b' ', b'|', b' ', b':', b' ', b'|', b' ', b':', b' '],\n",
       "       [b'Y', b'|', b' ', b':', b' ', b'|', b'B', b':', b' ']],\n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.desc[1:-1, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0 , 0 ) -  b'R'\n",
      "( 0 , 1 ) -  b':'\n",
      "( 0 , 2 ) -  b' '\n",
      "( 0 , 3 ) -  b'|'\n",
      "( 0 , 4 ) -  b' '\n",
      "( 0 , 5 ) -  b':'\n",
      "( 0 , 6 ) -  b' '\n",
      "( 0 , 7 ) -  b':'\n",
      "( 0 , 8 ) -  b'G'\n",
      "( 1 , 0 ) -  b' '\n",
      "( 1 , 1 ) -  b':'\n",
      "( 1 , 2 ) -  b' '\n",
      "( 1 , 3 ) -  b'|'\n",
      "( 1 , 4 ) -  b' '\n",
      "( 1 , 5 ) -  b':'\n",
      "( 1 , 6 ) -  b' '\n",
      "( 1 , 7 ) -  b':'\n",
      "( 1 , 8 ) -  b' '\n",
      "( 2 , 0 ) -  b' '\n",
      "( 2 , 1 ) -  b':'\n",
      "( 2 , 2 ) -  b' '\n",
      "( 2 , 3 ) -  b':'\n",
      "( 2 , 4 ) -  b' '\n",
      "( 2 , 5 ) -  b':'\n",
      "( 2 , 6 ) -  b' '\n",
      "( 2 , 7 ) -  b':'\n",
      "( 2 , 8 ) -  b' '\n",
      "( 3 , 0 ) -  b' '\n",
      "( 3 , 1 ) -  b'|'\n",
      "( 3 , 2 ) -  b' '\n",
      "( 3 , 3 ) -  b':'\n",
      "( 3 , 4 ) -  b' '\n",
      "( 3 , 5 ) -  b'|'\n",
      "( 3 , 6 ) -  b' '\n",
      "( 3 , 7 ) -  b':'\n",
      "( 3 , 8 ) -  b' '\n",
      "( 4 , 0 ) -  b'Y'\n",
      "( 4 , 1 ) -  b'|'\n",
      "( 4 , 2 ) -  b' '\n",
      "( 4 , 3 ) -  b':'\n",
      "( 4 , 4 ) -  b' '\n",
      "( 4 , 5 ) -  b'|'\n",
      "( 4 , 6 ) -  b'B'\n",
      "( 4 , 7 ) -  b':'\n",
      "( 4 , 8 ) -  b' '\n"
     ]
    }
   ],
   "source": [
    "cells = env.desc[1:-1, 1:-1]\n",
    "for x, row  in enumerate(cells):\n",
    "    for y, cell in enumerate(row):\n",
    "        print('(',x,',',y,') - ', cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without RL (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 3611\n",
      "Penalties incurred: 1105\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1002\n",
      "State: 196\n",
      "Action: 5\n",
      "Reward: -10\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning (Q-Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 1min 8s, sys: 14.9 s, total: 1min 23s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "%matplotlib inline\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2f860af7d2d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtotal_penalties\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpenalties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtotal_epochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results after {episodes} episodes:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_rewards' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward, rewards = 0, 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        rewards += reward\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    total_rewards += rewards\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "print(f\"Average reward per episode: {total_rewards / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Taxi-v3'\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 10)             5000      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 10,956\n",
      "Trainable params: 10,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(500, 10, input_length=1))\n",
    "model.add(Reshape((10,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=action_size, memory=memory, nb_steps_warmup=500, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /home/teto/.local/share/virtualenvs/mesa-keras-rl-lng-ZeAX/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "100000/100000 [==============================] - 653s 7ms/step - reward: -1.3310\n",
      "1065 episodes - episode_reward: -124.972 [-891.000, 14.000] - loss: 4.205 - mae: 25.872 - mean_q: -26.502 - prob: 1.000\n",
      "\n",
      "Interval 2 (100000 steps performed)\n",
      "100000/100000 [==============================] - 668s 7ms/step - reward: -0.3583\n",
      "4659 episodes - episode_reward: -7.689 [-630.000, 15.000] - loss: 0.896 - mae: 9.606 - mean_q: 3.227 - prob: 1.000\n",
      "\n",
      "Interval 3 (200000 steps performed)\n",
      "100000/100000 [==============================] - 700s 7ms/step - reward: 0.1609\n",
      "6798 episodes - episode_reward: 2.367 [-68.000, 15.000] - loss: 0.003 - mae: 7.362 - mean_q: 12.680 - prob: 1.000\n",
      "\n",
      "Interval 4 (300000 steps performed)\n",
      "100000/100000 [==============================] - 672s 7ms/step - reward: 0.1571\n",
      "6770 episodes - episode_reward: 2.321 [-93.000, 15.000] - loss: 0.002 - mae: 7.363 - mean_q: 12.690 - prob: 1.000\n",
      "\n",
      "Interval 5 (400000 steps performed)\n",
      "100000/100000 [==============================] - 699s 7ms/step - reward: 0.1548\n",
      "6777 episodes - episode_reward: 2.283 [-135.000, 15.000] - loss: 0.002 - mae: 7.359 - mean_q: 12.696 - prob: 1.000\n",
      "\n",
      "Interval 6 (500000 steps performed)\n",
      "100000/100000 [==============================] - 695s 7ms/step - reward: 0.1624\n",
      "6789 episodes - episode_reward: 2.392 [-87.000, 15.000] - loss: 0.001 - mae: 7.379 - mean_q: 12.732 - prob: 1.000\n",
      "\n",
      "Interval 7 (600000 steps performed)\n",
      "100000/100000 [==============================] - 834s 8ms/step - reward: 0.1761\n",
      "6814 episodes - episode_reward: 2.585 [-47.000, 15.000] - loss: 0.001 - mae: 7.373 - mean_q: 12.720 - prob: 1.000\n",
      "\n",
      "Interval 8 (700000 steps performed)\n",
      "100000/100000 [==============================] - 711s 7ms/step - reward: 0.1516\n",
      "6791 episodes - episode_reward: 2.232 [-135.000, 15.000] - loss: 0.001 - mae: 7.370 - mean_q: 12.726 - prob: 1.000\n",
      "\n",
      "Interval 9 (800000 steps performed)\n",
      "100000/100000 [==============================] - 728s 7ms/step - reward: 0.1577\n",
      "6765 episodes - episode_reward: 2.332 [-100.000, 15.000] - loss: 0.001 - mae: 7.347 - mean_q: 12.670 - prob: 1.000\n",
      "\n",
      "Interval 10 (900000 steps performed)\n",
      "100000/100000 [==============================] - 711s 7ms/step - reward: 0.1612\n",
      "done, took 7072.114 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d946cc550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1, nb_max_episode_steps=99, log_interval=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 6.000, steps: 15\n",
      "Episode 2: reward: 6.000, steps: 15\n",
      "Episode 3: reward: 8.000, steps: 13\n",
      "Episode 4: reward: 7.000, steps: 14\n",
      "Episode 5: reward: 5.000, steps: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d08f25910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5 , visualize=False, nb_max_episode_steps=99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('mesa-keras-rl': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "57ecadd80644193cdd4321ea58c039ba219334e9a136e552e824e28ff833f489"
    }
   },
   "name": "Python 3.8.2 64-bit ('mesa-keras-rl': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}